{
  "bertscore": {
    "id": "bertscore",
    "name": "BertScore",
    "description": "BertScore uses Bert to check for the similarity in embedding between two sentences.",
    "hash": "0ea41c370f3628f6"
  },
  "spelling": {
    "id": "spelling",
    "name": "SpellingScore",
    "description": "SpellingScore uses Levenshetein Distance to find permutations within an edit distance of 2 form the original word before comparing to known words in a word frequency list.",
    "hash": "aec08a4d7680cc4e"
  },
  "gpt4annotator": {
    "id": "gpt4annotator",
    "name": "GPT4 Annotator",
    "description": "Calculates the number of refused response using GPT4.",
    "hash": "c42af994f3751c9d"
  },
  "bleuscore": {
    "id": "bleuscore",
    "name": "BleuScore",
    "description": "Bleuscore uses Bleu to return the various rouge scores.",
    "hash": "f403209817f242fb"
  },
  "readabilityscore": {
    "id": "readabilityscore",
    "name": "ReadabilityScore",
    "description": "ReadabilityScore uses Flesch Reading Ease to compute the complexity of the output",
    "hash": "3bd6f0f1faeba949"
  },
  "toxicity-classifier": {
    "id": "toxicity-classifier",
    "name": "Toxicity Classifier",
    "description": "This classifier measures how toxic a given input isand calculate the number of toxic sentence detected.",
    "hash": "6a8edb11ddbf7a3f"
  },
  "rougescorer": {
    "id": "rougescorer",
    "name": "RougeScorer",
    "description": "RougeScorer returns the various rouge scores.",
    "hash": "fdb2d50de8496556"
  },
  "advglue": {
    "id": "advglue",
    "name": "Attack Success Rate",
    "description": "Attack success rate measures how successful a changed prompt performs. A high score shows that the system under test is highly sensitive towards a prompt with minimal changes.",
    "hash": "b3d1182c60a88b54"
  },
  "leakagerate": {
    "id": "leakagerate",
    "name": "LeakageRate",
    "description": "Leakage Rate will compare the LCS between two string - Output and Target.",
    "hash": "f93b6e07306308d4"
  },
  "reverseexactstrmatch": {
    "id": "reverseexactstrmatch",
    "name": "ReverseExactStrMatch",
    "description": "ReverseExactStrMatch will compare the output from language model with the expected target.",
    "hash": "b4d8d8338b4ea4d4"
  },
  "exactstrmatch": {
    "id": "exactstrmatch",
    "name": "ExactStrMatch",
    "description": "ExactStrMatch will compare the output from language model with a single target or multiple expected target.",
    "hash": "990c50c7736d11db"
  },
  "relaxstrmatch": {
    "id": "relaxstrmatch",
    "name": "RelaxStrMatch",
    "description": "RelaxStrMatch will remove symbols and spaces before comparing the output from language model with the expected target.",
    "hash": "68fcf1f9dc064ffd"
  },
  "llamaguardannotator": {
    "id": "llamaguardannotator",
    "name": "LlamaGuard Annotator",
    "description": "Calculates the number of safe, unsafe and unknown response by using LlamaGuard.",
    "hash": "3e3e97205b4721d6"
  },
  "samplemetric": {
    "id": "samplemetric",
    "name": "SampleMetric",
    "description": "Sample Metric will provide examples on connecting to LLMs.",
    "hash": "c8d1e48a91f66eb8"
  },
  "singapore-toxicity-classifier": {
    "id": "singapore-toxicity-classifier",
    "name": "Singapore Toxicity Classifier",
    "description": "This classifier measures how unsafe a given input is in the Singapore context.\nIt can calculate the number of toxic sentences detected. ",
    "hash": "012e026bfa28ac13"
  }
}