import logging
from typing import Any

from moonshot.src.connectors.connector import Connector, perform_retry
from moonshot.src.connectors_endpoints.connector_endpoint_arguments import (
    ConnectorEndpointArguments,
)
from openai import AsyncOpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OpenAIConnector(Connector):
    def __init__(self, ep_arguments: ConnectorEndpointArguments):
        # Initialize super class
        super().__init__(ep_arguments)

        # Set OpenAI Key
        self._client = AsyncOpenAI(
            api_key=self.token,
            base_url=self.endpoint if self.endpoint and self.endpoint != "" else None,
        )

        # Set the model to use and remove it from optional_params if it exists
        self.model = self.optional_params.get("model", "")

    @Connector.rate_limited
    @perform_retry
    async def get_response(self, prompt: str) -> str:
        """
        Asynchronously sends a prompt to the OpenAI API and returns the generated response
        using streaming to handle partial responses as they are generated.

        Args:
            prompt (str): The input prompt to send to the OpenAI API.

        Returns:
            str: The concatenated text response generated by the OpenAI model.
        """
        optional_params = prompt.split()[0]

        try:
            list_of_params = optional_params.split(":")
            stream_value = list_of_params[2].split("=")[1]
            mimic_type = list_of_params[3].split("=")[1]
        except Exception as e:
            print("An error occurred:", str(e))

        connector_prompt = f"{self.pre_prompt}{prompt}{self.post_prompt}"
        if self.system_prompt:
            openai_request = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": connector_prompt},
            ]
        else:
            openai_request = [{"role": "user", "content": connector_prompt}]

        # Merge self.optional_params with additional parameters
        new_params = {
            **self.optional_params,
            "model": self.model,
            "messages": openai_request,
            "timeout": self.timeout,
        }

        if stream_value == "true":
            new_params["stream"] = True

            response_stream = await self._client.chat.completions.create(**new_params)
            full_response = ""
            async for chunk in response_stream:
                print("CHUNK:", chunk)
                if chunk.choices[0].delta.content:
                    if mimic_type == "api":
                        full_response += chunk.choices[0].delta.content
                    else:
                        full_response = chunk.choices[0].delta.content

            return full_response
        else:
            response = await self._client.chat.completions.create(**new_params)
            return await self._process_response(response)

    async def _process_response(self, response: Any) -> str:
        """
        Process the response from OpenAI's API and return the message content as a string.
        In streaming mode, this function is not used because processing is handled in the
        get_response function itself.

        Args:
            response (Any): The response object received from an OpenAI API call.

        Returns:
            str: A string containing the message content.
        """
        return response.choices[0].message.content
